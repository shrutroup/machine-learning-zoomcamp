{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrutroup/machine-learning-zoomcamp/blob/main/clothing_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/alexeygrigorev/clothing-dataset-small.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl36byRRU6zN",
        "outputId": "a3412e12-40de-416a-c6cf-ba908d38397e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'clothing-dataset-small'...\n",
            "remote: Enumerating objects: 3839, done.\u001b[K\n",
            "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
            "remote: Compressing objects: 100% (400/400), done.\u001b[K\n",
            "remote: Total 3839 (delta 9), reused 385 (delta 0), pack-reused 3439 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3839/3839), 100.58 MiB | 26.83 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "BJ-AjlD_Vhlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.xception import Xception\n",
        "from tensorflow.keras.applications.xception import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "7DqXS4azVyeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lu3URFCkV6MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 299\n",
        "\n",
        "train_gen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    shear_range=10,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_ds = train_gen.flow_from_directory(\n",
        "    './clothing-dataset-small/train',\n",
        "    target_size=(input_size, input_size),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "\n",
        "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "val_ds = train_gen.flow_from_directory(\n",
        "    './clothing-dataset-small/validation',\n",
        "    target_size=(input_size, input_size),\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X45gRvQtWBaT",
        "outputId": "b12e0686-21aa-4cb3-af64-33f2935adf37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3068 images belonging to 10 classes.\n",
            "Found 341 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "    'xception_v4_1_{epoch:02d}_{val_accuracy:.3f}.keras',\n",
        "    save_best_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max'\n",
        ")"
      ],
      "metadata": {
        "id": "tuxJyGx7WFIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(input_size=150, learning_rate=0.01, size_inner=100,\n",
        "               droprate=0.5):\n",
        "\n",
        "    base_model = Xception(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(input_size, input_size, 3)\n",
        "    )\n",
        "\n",
        "    base_model.trainable = False\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    inputs = keras.Input(shape=(input_size, input_size, 3))\n",
        "    base = base_model(inputs, training=False)\n",
        "    vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
        "\n",
        "    inner = keras.layers.Dense(size_inner, activation='relu')(vectors)\n",
        "    drop = keras.layers.Dropout(droprate)(inner)\n",
        "\n",
        "    outputs = keras.layers.Dense(10)(drop)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "REPu0MiuW8o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0005\n",
        "size = 100\n",
        "droprate = 0.2\n",
        "\n",
        "model = make_model(\n",
        "    input_size=input_size,\n",
        "    learning_rate=learning_rate,\n",
        "    size_inner=size,\n",
        "    droprate=droprate\n",
        ")\n",
        "\n",
        "history = model.fit(train_ds, epochs=50, validation_data=val_ds,\n",
        "                   callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_dAoTFkrWb-N",
        "outputId": "3d005db2-02ad-43c2-d63d-2f68519dc67f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 843ms/step - accuracy: 0.5803 - loss: 1.3574 - val_accuracy: 0.8446 - val_loss: 0.5358\n",
            "Epoch 2/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 752ms/step - accuracy: 0.8104 - loss: 0.5661 - val_accuracy: 0.8622 - val_loss: 0.4536\n",
            "Epoch 3/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 755ms/step - accuracy: 0.8565 - loss: 0.4200 - val_accuracy: 0.8680 - val_loss: 0.4051\n",
            "Epoch 4/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 751ms/step - accuracy: 0.8690 - loss: 0.3779 - val_accuracy: 0.8475 - val_loss: 0.4545\n",
            "Epoch 5/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 754ms/step - accuracy: 0.8803 - loss: 0.3737 - val_accuracy: 0.8886 - val_loss: 0.3829\n",
            "Epoch 6/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 747ms/step - accuracy: 0.8879 - loss: 0.3280 - val_accuracy: 0.8915 - val_loss: 0.3561\n",
            "Epoch 7/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 735ms/step - accuracy: 0.9049 - loss: 0.2746 - val_accuracy: 0.8768 - val_loss: 0.3754\n",
            "Epoch 8/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 742ms/step - accuracy: 0.9022 - loss: 0.2970 - val_accuracy: 0.8680 - val_loss: 0.3714\n",
            "Epoch 9/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 782ms/step - accuracy: 0.9157 - loss: 0.2485 - val_accuracy: 0.8798 - val_loss: 0.3662\n",
            "Epoch 10/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 731ms/step - accuracy: 0.9151 - loss: 0.2451 - val_accuracy: 0.8798 - val_loss: 0.3581\n",
            "Epoch 11/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 744ms/step - accuracy: 0.9250 - loss: 0.2118 - val_accuracy: 0.8944 - val_loss: 0.3378\n",
            "Epoch 12/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 754ms/step - accuracy: 0.9185 - loss: 0.2204 - val_accuracy: 0.8710 - val_loss: 0.3618\n",
            "Epoch 13/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 751ms/step - accuracy: 0.9393 - loss: 0.1995 - val_accuracy: 0.8798 - val_loss: 0.3948\n",
            "Epoch 14/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 727ms/step - accuracy: 0.9462 - loss: 0.1721 - val_accuracy: 0.8856 - val_loss: 0.3618\n",
            "Epoch 15/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 748ms/step - accuracy: 0.9373 - loss: 0.1890 - val_accuracy: 0.8680 - val_loss: 0.3989\n",
            "Epoch 16/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 746ms/step - accuracy: 0.9409 - loss: 0.1816 - val_accuracy: 0.8680 - val_loss: 0.3690\n",
            "Epoch 17/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 775ms/step - accuracy: 0.9436 - loss: 0.1716 - val_accuracy: 0.8622 - val_loss: 0.3885\n",
            "Epoch 18/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 730ms/step - accuracy: 0.9514 - loss: 0.1591 - val_accuracy: 0.8651 - val_loss: 0.3662\n",
            "Epoch 19/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 746ms/step - accuracy: 0.9541 - loss: 0.1352 - val_accuracy: 0.8798 - val_loss: 0.3649\n",
            "Epoch 20/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 746ms/step - accuracy: 0.9571 - loss: 0.1301 - val_accuracy: 0.8739 - val_loss: 0.3814\n",
            "Epoch 21/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 751ms/step - accuracy: 0.9642 - loss: 0.1174 - val_accuracy: 0.8768 - val_loss: 0.3580\n",
            "Epoch 22/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 739ms/step - accuracy: 0.9533 - loss: 0.1355 - val_accuracy: 0.8680 - val_loss: 0.4106\n",
            "Epoch 23/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 754ms/step - accuracy: 0.9652 - loss: 0.1207 - val_accuracy: 0.8974 - val_loss: 0.3638\n",
            "Epoch 24/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 747ms/step - accuracy: 0.9583 - loss: 0.1287 - val_accuracy: 0.8651 - val_loss: 0.3968\n",
            "Epoch 25/50\n",
            "\u001b[1m 1/96\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17:04\u001b[0m 11s/step - accuracy: 1.0000 - loss: 0.0427"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-952730532.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m history = model.fit(train_ds, epochs=50, validation_data=val_ds,\n\u001b[0m\u001b[1;32m     13\u001b[0m                    callbacks=[checkpoint])\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[1;32m    220\u001b[0m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mhas_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       return gen_optional_ops.optional_has_value(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    173\u001b[0m         _ctx, \"OptionalHasValue\", name, optional)\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g5bcQLg6XBHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70150728"
      },
      "source": [
        "# Task\n",
        "Convert the provided TensorFlow/Keras code for an image classification model using Xception to PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "644095d9"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "### Subtask:\n",
        "Convert the data loading and preprocessing steps from `ImageDataGenerator` to PyTorch's `Dataset` and `DataLoader`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "776969cc"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary modules from PyTorch for data handling and image transformations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1520a78c"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a33fb15c"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a custom PyTorch Dataset class to load images from directories, apply transformations, and provide image-label pairs. Implement the `__len__` and `__getitem__` methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb1016cc"
      },
      "source": [
        "class ClothingDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.classes = sorted(os.listdir(data_dir))\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
        "\n",
        "        for label_name in self.classes:\n",
        "            label_dir = os.path.join(data_dir, label_name)\n",
        "            for img_name in os.listdir(label_dir):\n",
        "                self.image_paths.append(os.path.join(label_dir, img_name))\n",
        "                self.labels.append(self.class_to_idx[label_name])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "012c9586"
      },
      "source": [
        "**Reasoning**:\n",
        "Define image transformations for both training and validation datasets, including preprocessing based on Xception's requirements. Then, create instances of the `ClothingDataset` and `DataLoader` for both sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6513076b"
      },
      "source": [
        "input_size = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e911db4",
        "outputId": "df77729d-728e-4b73-a913-23f41d147189"
      },
      "source": [
        "# Define image transformations for training with data augmentation and preprocessing\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    # image transformations\n",
        "    transforms.RandomRotation(10), # Equivalent to shear_range\n",
        "    transforms.RandomResizedCrop(input_size, scale=(0.9, 1.0)), # Equivalent to zoom_range\n",
        "    transforms.RandomHorizontalFlip(), # Equivalent to horizontal_flip\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # preprocessing\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Define image transformations for validation with preprocessing\n",
        "val_transforms = transforms.Compose([\n",
        "    # image transformations\n",
        "    transforms.Resize((input_size, input_size)), # Resize to the target size\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # preprocessing\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Create instances of the custom dataset for training and validation\n",
        "train_dataset = ClothingDataset(\n",
        "    data_dir='./clothing-dataset-small/train',\n",
        "    transform=train_transforms\n",
        ")\n",
        "\n",
        "val_dataset = ClothingDataset(\n",
        "    data_dir='./clothing-dataset-small/validation',\n",
        "    transform=val_transforms\n",
        ")\n",
        "\n",
        "# Create DataLoaders for iterating through the datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of validation batches: {len(val_loader)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training batches: 96\n",
            "Number of validation batches: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57b66acb"
      },
      "source": [
        "## Model Definition\n",
        "\n",
        "### Subtask:\n",
        "Translate the Xception model and the custom layers from TensorFlow/Keras to PyTorch modules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66f10c98"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary modules from PyTorch for defining the neural network, including a pre-trained Xception model from `torchvision.models`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "626a87b4"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a0a9bfa"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the PyTorch model based on the structure of the Keras model: load a pre-trained Xception model, remove its original classification head, add a global average pooling layer, a dense inner layer with ReLU activation and dropout, and a final dense output layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb885a4d"
      },
      "source": [
        "**Reasoning**:\n",
        "Create an instance of the PyTorch model and move it to the appropriate device (GPU if available)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d280072"
      },
      "source": [
        "## Model Definition (using MobileNetV2)\n",
        "\n",
        "### Subtask:\n",
        "Translate the model definition to use MobileNetV2 from `torchvision.models`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e68cda7"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary modules from PyTorch for defining the neural network and MobileNetV2 from `torchvision.models`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df2dff9f"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the PyTorch model using a pre-trained MobileNetV2 model, replace its classifier, and add the necessary layers for the clothing classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f17e0c5"
      },
      "source": [
        "class ClothingClassifierMobileNet(nn.Module):\n",
        "    def __init__(self, size_inner=100, droprate=0.2, num_classes=10):\n",
        "        super(ClothingClassifierMobileNet, self).__init__()\n",
        "        # Load pre-trained MobileNetV2 model\n",
        "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
        "\n",
        "        # Replace the original classifier\n",
        "        # The original classifier in MobileNetV2 starts with a Conv2d layer\n",
        "        # followed by a Linear layer. We need to replace the entire classifier.\n",
        "        # The input features to the classifier are 1280 for MobileNetV2.\n",
        "        self.base_model.classifier = nn.Identity() # Remove the original classifier\n",
        "\n",
        "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.inner = nn.Linear(1280, size_inner) # 1280 is the number of output features from MobileNetV2 features\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(droprate)\n",
        "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model.features(x) # Access features attribute for MobileNetV2\n",
        "        x = self.global_avg_pooling(x)\n",
        "        x = torch.flatten(x, 1) # Flatten the output from pooling\n",
        "        x = self.inner(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89b9e6c9"
      },
      "source": [
        "**Reasoning**:\n",
        "Create an instance of the MobileNetV2-based PyTorch model and move it to the appropriate device (GPU if available)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "size = 32 # Corresponds to size_inner in the TensorFlow code\n",
        "droprate = 0.2 # Corresponds to droprate in the TensorFlow code"
      ],
      "metadata": {
        "id": "hVt_JU-bJAhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a567ac4b",
        "outputId": "31ca2789-7ffa-4f88-8e8e-6d3a70ed3ad8"
      },
      "source": [
        "model = ClothingClassifierMobileNet(size_inner=size, droprate=droprate, num_classes=len(train_dataset.classes))\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model is on: {device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is on: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92c55e5e"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Adapt the training loop, including the optimizer, loss function, and model training steps, from Keras's `model.fit` to a PyTorch training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc211341"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary PyTorch modules for optimization and loss calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28d1997d"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f36ded9"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the optimizer and the loss function for the model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ac3fddf"
      },
      "source": [
        "learning_rate = 0.0001\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# Use CrossEntropyLoss for classification with logits\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48ab61f9"
      },
      "source": [
        "## Model Checkpointing\n",
        "\n",
        "### Subtask:\n",
        "Implement model checkpointing in PyTorch equivalent to the Keras `ModelCheckpoint` callback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4618bf5c"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the training loop, including iterating over epochs and batches, calculating loss and accuracy, performing backpropagation, and updating model weights. Also, include the logic for saving the best model based on validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "d078751b",
        "outputId": "e62311d9-07c1-49ed-bf27-c27c6f4903b4"
      },
      "source": [
        "num_epochs = 50\n",
        "best_val_accuracy = 0.0\n",
        "checkpoint_path = 'mobilenet_v2_v1_{epoch:02d}_{val_accuracy:.3f}.pth'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = correct_predictions / total_predictions\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}')\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    val_correct_predictions = 0\n",
        "    val_total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during validation\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total_predictions += labels.size(0)\n",
        "            val_correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = val_correct_predictions / val_total_predictions\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # Checkpoint logic\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        # Format the checkpoint filename with epoch and accuracy\n",
        "        current_checkpoint_path = checkpoint_path.format(epoch=epoch+1, val_accuracy=val_accuracy)\n",
        "        torch.save(model.state_dict(), current_checkpoint_path)\n",
        "        print(f'Checkpoint saved to {current_checkpoint_path}')\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 1.4331, Train Accuracy: 0.5636\n",
            "Epoch 1/50, Val Loss: 0.7555, Val Accuracy: 0.7918\n",
            "Checkpoint saved to mobilenet_v2_v1_01_0.792.pth\n",
            "Epoch 2/50, Train Loss: 0.6110, Train Accuracy: 0.8328\n",
            "Epoch 2/50, Val Loss: 0.3643, Val Accuracy: 0.8915\n",
            "Checkpoint saved to mobilenet_v2_v1_02_0.891.pth\n",
            "Epoch 3/50, Train Loss: 0.3501, Train Accuracy: 0.9045\n",
            "Epoch 3/50, Val Loss: 0.2817, Val Accuracy: 0.9208\n",
            "Checkpoint saved to mobilenet_v2_v1_03_0.921.pth\n",
            "Epoch 4/50, Train Loss: 0.2480, Train Accuracy: 0.9329\n",
            "Epoch 4/50, Val Loss: 0.3111, Val Accuracy: 0.8886\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1237961924.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtotal_predictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "964712d6"
      },
      "source": [
        "## Export PyTorch model to ONNX\n",
        "\n",
        "### Subtask:\n",
        "Convert the trained PyTorch model to the ONNX format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7618babd"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the best saved PyTorch model, set it to evaluation mode, and export it to an ONNX file using `torch.onnx.export`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7gxvfjOJT9f",
        "outputId": "70d1fb8b-2e62-44e0-bde4-1c28925119d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best checkpoint file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "list_of_files = glob.glob('mobilenet_v2_v1_*.pth')\n",
        "latest_file = max(list_of_files, key=os.path.getctime)\n",
        "print(f\"Loading the best model from: {latest_file}\")\n",
        "\n",
        "# latest_file = 'mobilenet_0.889.pth'\n",
        "\n",
        "model = ClothingClassifierMobileNet(size_inner=size, droprate=droprate, num_classes=len(train_dataset.classes))\n",
        "model.load_state_dict(torch.load(latest_file))\n",
        "model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jDwvAI_YRODz",
        "outputId": "fa9c5725-7bcd-495d-86a5-4119e7f37587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the best model from: mobilenet_v2_v1_03_0.921.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = ClothingDataset(\n",
        "    data_dir='./clothing-dataset-small/test',\n",
        "    transform=val_transforms,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "ogjLNYcMR6qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, labels in test_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    break"
      ],
      "metadata": {
        "id": "HwQ0g-7-RS3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPUQc8VmRlm6",
        "outputId": "b95c3b2a-bca8-43d1-870f-b01bb31fca97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dMQFLTZRxyM",
        "outputId": "c4602b97-2ba3-42a6-ce0d-5c421b34d2a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 6, 2, 0, 6, 4, 2, 3, 9, 2, 3, 4, 7, 6, 9, 6, 9, 4, 3, 1, 2, 6, 6, 4,\n",
              "        4, 2, 5, 2, 2, 9, 9, 6], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(inputs)\n",
        "pred_labels = torch.max(outputs, 1).indices\n",
        "(labels == pred_labels).sum() / len(pred_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX8wFlf-SMCG",
        "outputId": "374bb467-38a1-427f-e4ad-134026fbe970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9688, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-image-helper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VUeDg1WSrRg",
        "outputId": "6ca1c23b-509d-489d-9b78-486a02b515c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-image-helper in /usr/local/lib/python3.11/dist-packages (0.0.2)\n",
            "Requirement already satisfied: numpy>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from keras-image-helper) (2.3.2)\n",
            "Requirement already satisfied: pillow>=11.3.0 in /usr/local/lib/python3.11/dist-packages (from keras-image-helper) (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_image_helper import create_preprocessor"
      ],
      "metadata": {
        "id": "CNM9xr3hTLci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_pytorch_style(X):\n",
        "    # X: shape (1, 299, 299, 3), dtype=float32, values in [0, 255]\n",
        "    X = X / 255.0\n",
        "\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n",
        "\n",
        "    # Convert NHWC → NCHW\n",
        "    # from (batch, height, width, channels) → (batch, channels, height, width)\n",
        "    X = X.transpose(0, 3, 1, 2)\n",
        "\n",
        "    # Normalize\n",
        "    X = (X - mean) / std\n",
        "\n",
        "    return X.astype(np.float32)"
      ],
      "metadata": {
        "id": "hW7aIJBbWt4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_image_helper import create_preprocessor"
      ],
      "metadata": {
        "id": "2y-2qMyiWoYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = create_preprocessor(preprocess_pytorch_style, target_size=(224, 224))"
      ],
      "metadata": {
        "id": "8CLIhyAzS--V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://bit.ly/mlbookcamp-pants'\n",
        "X = preprocessor.from_url(url)"
      ],
      "metadata": {
        "id": "rYgTt5ikTr3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXVJuOnZTyQY",
        "outputId": "6f048aba-574d-4012-d35e-9a4f5bb7178d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3, 224, 224)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.Tensor(X).to(device)"
      ],
      "metadata": {
        "id": "OBlt_bz7USbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.array(model(X).data[0].cpu())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfqFCo-jUJPM",
        "outputId": "a3e65397-9356-42b8-bd76-a9dfbfecb378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3451073690.py:1: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
            "  pred = np.array(model(X).data[0].cpu())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"dress\",\n",
        "    \"hat\",\n",
        "    \"longsleeve\",\n",
        "    \"outwear\",\n",
        "    \"pants\",\n",
        "    \"shirt\",\n",
        "    \"shoes\",\n",
        "    \"shorts\",\n",
        "    \"skirt\",\n",
        "    \"t-shirt\",\n",
        "]\n",
        "\n",
        "dict(zip(classes, pred.tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3Uzx4wlUtcC",
        "outputId": "97dbaf96-0988-4472-9a7d-9b81871eef67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dress': 0.14996567368507385,\n",
              " 'hat': -1.7628498077392578,\n",
              " 'longsleeve': -3.3368780612945557,\n",
              " 'outwear': -1.7682585716247559,\n",
              " 'pants': 5.336868762969971,\n",
              " 'shirt': -1.0429128408432007,\n",
              " 'shoes': -0.3072749376296997,\n",
              " 'shorts': 0.7430807948112488,\n",
              " 'skirt': -1.7061662673950195,\n",
              " 't-shirt': -3.9114773273468018}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e1163df",
        "outputId": "dfa2abd4-4ed7-435f-a3d0-83e3ba0e0f46"
      },
      "source": [
        "# Define dummy input for ONNX export\n",
        "# The input shape should match the input shape of your model (batch_size, channels, height, width)\n",
        "# Use a batch size of 1 for simplicity when exporting\n",
        "dummy_input = torch.randn(1, 3, input_size, input_size).to(device)\n",
        "\n",
        "# Export the model to ONNX format\n",
        "onnx_path = \"clothing_classifier_mobilenet_v2_latest.onnx\"\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,                     # PyTorch Model\n",
        "    dummy_input,               # Dummy input tensor\n",
        "    onnx_path,                 # Path to save the ONNX model\n",
        "    verbose=True,              # Print export details\n",
        "    input_names=['input'],     # Input layer name\n",
        "    output_names=['output'],   # Output layer name\n",
        "    dynamic_axes={             # Dynamic batch size\n",
        "        'input' : {0 : 'batch_size'},\n",
        "        'output' : {0 : 'batch_size'}\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Model exported to {onnx_path}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to clothing_classifier_mobilenet_v2_latest.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GfS3FY62ZVA9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}